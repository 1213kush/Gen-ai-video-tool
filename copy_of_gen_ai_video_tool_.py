# -*- coding: utf-8 -*-
"""Copy of Gen ai video tool .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12AeIWd9WjPNg2WknfIaiUSpoSTCUK8AF

! hare krishana üôè

We  need Google news api for search any article that published in news source and blogs in last 5 years for topic
"""

import requests

url = 'https://newsapi.org/v2/top-headlines'
params = {
    'country': 'in',
    'apiKey': '48d0ce4dd4ce437baa8c951731b77ecd'
}

response = requests.get(url, params=params)
data = response.json()

"""i want to show top of 10 artical"""

response = requests.get(url, params=params)
data = response.json()

# Assuming 'data' contains a list of articles under a key like 'articles'
for article in data.get('articles', []):  # Safely access 'articles' or an empty list if not present
    print({
        "title": article['title'],
        "summary": article['description'],
        "source": article['source']['name']
    })

"""we need pytrends lib for automatic downloading report from google trends"""

!pip install pytrends

from pytrends.request import TrendReq
import time

"""! set ist time zone for india tz = 330"""

! set ist time zone for india tz = 330
pytrends = TrendReq(hl='en-IN', tz=330)

pytrends.build_payload(['IPL', 'Lok Sabha', 'Virat Kohli', 'Delhi','Entertainment' ], geo='IN')

"""Get interest over time data and i want to fatch trending searches in india under the most popular key words"""

interest_over_time_df = pytrends.interest_over_time()

print(interest_over_time_df.head(5))
print(interest_over_time_df.tail(5))

"""i want to fatch trending searches in india under the most popular key words  by using serp api ."""

!pip install google-search-results

"""here google trends not working very efficent so that we need something that is free to use . so we found serpapi"""

import requests


API_KEY = '63ea5e1c9fa7be370b5c62e4523ef2889daf4b4469e022517e5a4b8329d9e8ec'

url = f"https://serpapi.com/search?q=trending+searches+in+India&api_key={API_KEY}"

response = requests.get(url)

print(f"Status Code: {response.status_code}")

print("Raw Response Text:", response.text)

"""Try to parse the response as JSON"""

try:
    data = response.json()
    print("Parsed JSON Response:", data)
except Exception as e:
    print("Error parsing JSON:", e)

"""feedparser is a library that is used to fetch trending search in many countries , and this library is used to extract data without using pytrends."""

!pip install feedparser

"""we need to handle the structure so that we need to use pandas"""

!pip install pandas

"""actully google trends full is not working so can find only the intrest key words trends ."""

from pytrends.request import TrendReq
import pandas as pd

def get_interest_trends(country_code='IN', keywords=['cricket', 'modi', 'ipl', 'delhi', 'election'], limit=5):
    pytrends = TrendReq(hl='en-US', tz=330)
    trend_scores = {}
    for kw in keywords:
        try:
            pytrends.build_payload([kw], geo=country_code.upper(), timeframe='now 7-d')
            data = pytrends.interest_over_time()
            if not data.empty:
                avg_score = int(data[kw].mean())
                trend_scores[kw] = avg_score
        except Exception as e:
            print(f"[!] Skipping keyword '{kw}' due to error: {e}")
    sorted_trends = sorted(trend_scores.items(), key=lambda x: x[1], reverse=True)
    return [f"{kw} (score: {score})" for kw, score in sorted_trends[:limit]] if sorted_trends else ["No trends available."]

"""For India = 'IN' united state ='US', united kingdon= 'GB' austrilia = 'AU', like all country name is used first two latter"""

trending_now = get_interest_trends('IN')
print(trending_now)

"""we want to use with free api's that will provide me best content ."""

import requests
from bs4 import BeautifulSoup

"""*itali!url of free and unofficial trends sites such as trends24.in this is like twitter .cized text*"""

url = 'https://trends24.in/'

"""make the request"""

response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

"""lets find the first trends section"""

trend_section = soup.find('ol', class_='trend-card__list')

print(" Top Twitter and other  Trends Right Now here :")
for li in trend_section.find_all('li')[:10]:
    trend = li.get_text(strip=True)
    print("-", trend)

"""hence we need to fatch more info  , we need to look reddit api but we are using colab , Asynchronous Environment , so need to install nest-asyncio"""

!pip install asyncio

"""for scrap the trend topic so we are using raddit . due to if any content is not present on previous then it will may be efficent to produce the data and content"""

!pip install asyncpraw nest_asyncio

import nest_asyncio
import asyncio
import asyncpraw

nest_asyncio.apply()

async def fetch_reddit():
    reddit = asyncpraw.Reddit(
        client_id="qPjiEvDQ2GQO4UXP5IgnAA",
        client_secret="3M-aJTRt9tKXH-zo_CEBDoOYpisb6w",
        user_agent="trending_bot by Ankush"
    )

    print(reddit.read_only)

!pip install asyncpraw nest_asyncio


import nest_asyncio
import asyncio
import asyncpraw

nest_asyncio.apply()


async def fetch_reddit():

    reddit = asyncpraw.Reddit(
        client_id="qPjiEvDQ2GQO4UXP5IgnAA",
        client_secret="3M-aJTRt9tKXH-zo_CEBDoOYpisb6w",
        user_agent="trending_bot_by_ankush"
    )

    subreddit = await reddit.subreddit("news")

    async for post in subreddit.hot(limit=10):
        print({
            "title": post.title,
            "summary": post.selftext[:100] if post.selftext else "No summary available",
            "source": f"Reddit - r/{subreddit.display_name}"
        })


await fetch_reddit()

"""# Generate Script from news

my aim is to convert article into 5-6 line video script , so we need to use open ai api

now , i have to creating a function to generate video script
"""

!pip install transformers

"""at this stage GPU will must be required so we need to use gpu as well as cpu resource ."""

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

"""now try to generate scripit on the base of topic

and create pipeline thats why generated text will automticaaly fatch
"""

from transformers import pipeline

summarizer = pipeline("text-generation", model="gpt2", truncation=True)
summarizer.tokenizer.pad_token = summarizer.tokenizer.eos_token

def generate_script_format(summary):
    print("## Key Points for Video:")
    print("---")

    prompt = f"""
    Generate 5-6 key points for a 60-second video explainer about this topic:

    {summary}
    """
    result = summarizer(prompt, max_length=200, min_length=120, do_sample=True, temperature=0.7)[0]['generated_text']
    lines = result.strip().split('\n')
    key_points = "\n".join(lines[1:])
    return key_points

news_summary = "About python"
key_points = generate_script_format(news_summary)
print("\nüé¨ Video Key Points:\n")
print(key_points)

"""# **Step 3:**-now i got text of 5-6 lines and this is sufficent for 30 sec of short video .

here intregation is most important due to this generated text will automatically fatch . and make it pipeline an we using elevenlabs .
"""

!pip install requests beautifulsoup4

!pip install requests

import requests


ELEVENLABS_API_KEY = "sk_69adee2803cb498efd5a9a34dc914674bb3b056055fea80a"
VOICE_ID = "EXAVITQu4vr4xnSDxMaL"  # Rachel or use any other available
SCRIPT_TEXT = """
I'm a Python programmer and a video game designer. I've been developing video game engines for many years. I developed several games for Nintendo, Atari, Sega, and various other companies. I'm also a student at the University of Michigan, and currently hold a Master's Degree in Computer Science from the University of Michigan. I am currently working on a video game about how to code in Python.

To learn about Python, see this book by Stephen King.

I'm a native of Montreal, Canada. I've been doing video game development for almost 20 years. I'm a native of Montreal, Canada since 1998. I started working on video game development in 1988. I've been developing video game engines for many years. I'm also a student at the University of Michigan, and currently hold a Master
"""

def generate_voice(text, filename="script_voice.mp3"):
    print(" Generating voice from your script...")

    headers = {
        "xi-api-key": ELEVENLABS_API_KEY,
        "Content-Type": "application/json"
    }

    payload = {
        "text": text,
        "model_id": "eleven_monolingual_v1",
        "voice_settings": {
            "stability": 0.75,
            "similarity_boost": 0.75
        }
    }

    response = requests.post(
        f"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}",
        headers=headers,
        json=payload
    )

    if response.status_code == 200:
        with open(filename, "wb") as f:
            f.write(response.content)
        print(f"‚úÖ Voice-over saved as: {filename}")
    else:
        print(f"‚ùå Voice generation failed: {response.status_code}")
        print(response.text)

""" RUN

"""

generate_voice(SCRIPT_TEXT)

"""here elevenlab free quota is full so that we need to use google text to voice .

#step 4 - create a pipe line and generate the audio . and video then combine
"""

!pip install gTTS

!pip install transformers torch gtts opencv-python numpy moviepy Pillow

"""import all lib due to it will be upload everything ."""

from transformers import pipeline
from gtts import gTTS
import os
import cv2
import numpy as np
from moviepy.editor import VideoClip, AudioFileClip, ColorClip
from PIL import Image, ImageDraw, ImageFont
import time

""" Set up the text generation pipeline and in this class i will use gpt-2 model"""

def initialize_text_generator():
    print("Initializing text generator...")
    text_generator = pipeline("text-generation", model="gpt2", truncation=True)
    text_generator.tokenizer.pad_token = text_generator.tokenizer.eos_token
    return text_generator

"""create a calss to generate text and keep the max limit is 300 , this length is can be changed."""

def generate_text(prompt, max_length=300):
    """Generate text using the transformer model"""
    text_generator = initialize_text_generator()
    print(f"Generating text based on prompt: '{prompt}'...")

    result = text_generator(
        prompt,
        max_length=max_length,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.95,
        repetition_penalty=1.2
    )[0]['generated_text']

    return result

"""now this class text_to_speech is converted text to speech"""

def text_to_speech(text, output_file="output_voice.mp3"):
    """Convert text to speech and save as MP3"""
    tts = gTTS(text)
    tts.save(output_file)
    print(f"‚úÖ Voice saved to {output_file}!")
    return output_file

"""this calss will maintain the screen text , color , bg"""

def create_text_image(text, width, height, style="modern"):
    """we try to create image by using pil"""
    if style == "modern":



        # Dark blue bg
        bg_color = (25, 25, 112)
        text_color = (255, 255, 255)  # White text



    else:
        # Light bg
        bg_color = (255, 255, 255)
        text_color = (0, 0, 0)  # Black text




    #make it simple
    img = Image.new('RGB', (width, height), bg_color)
    draw = ImageDraw.Draw(img)


    # Try to use a common font or default
    try:
        font = ImageFont.truetype("DejaVuSans.ttf", 36)
    except IOError:
        try:
            font = ImageFont.truetype("Arial.ttf", 36)
        except IOError:
            font = ImageFont.load_default()



             # Wrap and center text
    padding = 100
    max_width = width - (padding * 2)

    words = text.split()
    lines = []
    current_line = []

    for word in words:
        test_line = ' '.join(current_line + [word])
        try:
            # For new pil versions
            bbox = draw.textbbox((0, 0), test_line, font=font)
            w = bbox[2] - bbox[0]
        except AttributeError:
            try:
                # For older pil versions
                w, h = draw.textsize(test_line, font=font)
            except:
                # Ultimate fallback
                w = len(test_line) * 20

        if w <= max_width:
            current_line.append(word)
        else:
            lines.append(' '.join(current_line))
            current_line = [word]

    if current_line:
        lines.append(' '.join(current_line))





        # Draw text centered
    y_position = (height - (len(lines) * 50)) // 2
    for line in lines:
        try:
            # For newer pil versions
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
        except AttributeError:
            try:
                # For older pil versions
                text_width, _ = draw.textsize(line, font=font)
            except:
                # Ultimate fallback
                text_width = len(line) * 20

        x_position = (width - text_width) // 2
        draw.text((x_position, y_position), line, font=font, fill=text_color)
        y_position += 50




          # Draw a decorative line under text if modern style
    if style == "modern":
        line_y = y_position + 20
        line_width = width // 3
        line_start = (width - line_width) // 2
        draw.rectangle([(line_start, line_y), (line_start + line_width, line_y + 5)],
                      fill=(255, 165, 0))  # Orange line

    return np.array(img)

def create_animated_video(text, audio_file, output_file="output_video.mp4", style="modern"):
    """Create an animated video with text and audio using PIL for rendering"""
    print("Creating animated video...")

    # get video duration in time
    audio = AudioFileClip(audio_file)
    audio_duration = audio.duration

    # Video dimensions
    width, height = 1280, 720

    # Split text into sentences for individual slides
    sentences = [s.strip() for s in text.replace('\n', ' ').split('.') if s.strip()]
    if not sentences:
        sentences = ["No text generated"]

    # Calculate how long each sentence should be displayed
    sentence_duration = audio_duration / len(sentences)


     # Create a function that returns the frame at time t
    def make_frame(t):

        # Determine which sentence to show based on time
        sentence_idx = min(int(t / sentence_duration), len(sentences) - 1)
        sentence = sentences[sentence_idx]

        # Get time within current sentence (for fade effects)
        local_t = t - (sentence_idx * sentence_duration)
        fade_duration = 0.5

        # Create base frame with text
        frame = create_text_image(sentence, width, height, style)

        # Apply fade in/out effect
        if local_t < fade_duration:
            # Fade in
            alpha = local_t / fade_duration
            frame = frame * alpha
        elif local_t > sentence_duration - fade_duration:
            # Fade out
            alpha = (sentence_duration - local_t) / fade_duration
            frame = frame * alpha

        return frame.astype('uint8')

    # Create video clip
    video_clip = VideoClip(make_frame, duration=audio_duration)

    # Add audio
    final_video = video_clip.set_audio(audio)

    # Write the result to a file
    final_video.write_videofile(output_file, fps=24, codec='libx264')

    print(f"‚úÖ Video saved to {output_file}!")
    return output_file

def generate_video_from_prompt(prompt="Python is a programming language that",
                              output_path="output_video.mp4",
                              max_text_length=100,
                              style="modern"):
    """Generate a video with animated text and speech from a prompt"""
    print(f"Starting pipeline with prompt: '{prompt}'")
    start_time = time.time()

    # Step 1: Generate text
    generated_text = generate_text(prompt, max_text_length)
    print("\n===== Generated Text =====")
    print(generated_text)
    print("=========================\n")

    # Step 2: Convert text to speech
    audio_file = text_to_speech(generated_text)

    # Step 3: Create animated video with text and audio
    video_file = create_animated_video(generated_text, audio_file, output_path, style)

    result = {
        "text": generated_text,
        "audio": audio_file,
        "video": video_file
    }

    elapsed_time = time.time() - start_time
    print(f"Video generation complete in {elapsed_time:.2f} seconds! Your video is saved at: {result['video']}")
    return result

# Function to display video in Colab
def show_video(file_path):
    from IPython.display import HTML
    from base64 import b64encode

    mp4 = open(file_path, 'rb').read()
    data_url = f"data:video/mp4;base64,{b64encode(mp4).decode()}"
    return HTML(f"""
    <video width="640" height="360" controls>
        <source src="{data_url}" type="video/mp4">
    </video>
    """)

# Generate an animated video with the modern style
result = generate_video_from_prompt(
    prompt="How to learn python",

    output_path="animated_video.mp4",
    max_text_length=100,
    style="modern"  # Options: "modern" or "classic"
)

# Display the video
show_video("animated_video.mp4")

"""above this is not desarible"""

from transformers import pipeline
from gtts import gTTS
import os
import cv2
import numpy as np
from moviepy.editor import VideoClip, AudioFileClip, ColorClip, VideoFileClip, clips_array, CompositeVideoClip
from PIL import Image, ImageDraw, ImageFont
import time
import requests
import base64
import json

def install_dependencies():
    """Install required packages if not already installed"""
    try:
        import transformers
        import gtts
        import cv2
        import numpy
        import moviepy
        import PIL
        import requests
    except ImportError:
        print("Installing required packages...")
        import subprocess
        subprocess.check_call(["pip", "install", "transformers", "gtts", "opencv-python",
                              "numpy", "moviepy", "Pillow", "requests"])
        print("Packages installed successfully!")

# Set up the text generation pipeline
def initialize_text_generator():
    print("Initializing text generator...")
    text_generator = pipeline("text-generation", model="gpt2", truncation=True)
    text_generator.tokenizer.pad_token = text_generator.tokenizer.eos_token
    return text_generator

def generate_text(prompt, max_length=300):
    """Generate text using the transformer model"""
    text_generator = initialize_text_generator()
    print(f"Generating text based on prompt: '{prompt}'...")

    result = text_generator(
        prompt,
        max_length=max_length,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.95,
        repetition_penalty=1.2
    )[0]['generated_text']

    return result

def text_to_speech(text, output_file="output_voice.mp3"):
    """Convert text to speech and save as MP3"""
    tts = gTTS(text)
    tts.save(output_file)
    print(f"‚úÖ Voice saved to {output_file}!")
    return output_file

def create_text_image(text, width, height, style="modern"):
    """Create an image with text using PIL"""
    if style == "modern":
        # Dark blue background
        bg_color = (25, 25, 112)
        text_color = (255, 255, 255)  # White text
    else:
        # Light background
        bg_color = (255, 255, 255)
        text_color = (0, 0, 0)  # Black text

    # Create a blank image
    img = Image.new('RGB', (width, height), bg_color)
    draw = ImageDraw.Draw(img)

    # Try to use a common font or default
    try:
        font = ImageFont.truetype("DejaVuSans.ttf", 36)
    except IOError:
        try:
            font = ImageFont.truetype("Arial.ttf", 36)
        except IOError:
            font = ImageFont.load_default()



    # Wrap and center text
    padding = 100
    max_width = width - (padding * 2)

    words = text.split()
    lines = []
    current_line = []

    for word in words:
        test_line = ' '.join(current_line + [word])
        try:
            # For newer PIL versions
            bbox = draw.textbbox((0, 0), test_line, font=font)
            w = bbox[2] - bbox[0]
        except AttributeError:
            try:
                # For older PIL versions
                w, h = draw.textsize(test_line, font=font)
            except:
                # Ultimate fallback
                w = len(test_line) * 20

        if w <= max_width:
            current_line.append(word)
        else:
            lines.append(' '.join(current_line))
            current_line = [word]

    if current_line:
        lines.append(' '.join(current_line))



    # Draw text centered
    y_position = (height - (len(lines) * 50)) // 2
    for line in lines:
        try:
            # For newer PIL versions
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
        except AttributeError:
            try:
                # For older PIL versions
                text_width, _ = draw.textsize(line, font=font)
            except:
                # Ultimate fallback
                text_width = len(line) * 20


        x_position = (width - text_width) // 2
        draw.text((x_position, y_position), line, font=font, fill=text_color)
        y_position += 50


    # Draw a decorative line under text if modern style
    if style == "modern":
        line_y = y_position + 20
        line_width = width // 3
        line_start = (width - line_width) // 2
        draw.rectangle([(line_start, line_y), (line_start + line_width, line_y + 5)],
                      fill=(255, 165, 0))  # Orange line

    return np.array(img)

def create_animated_video(text, audio_file, output_file="output_video.mp4", style="modern"):
    """Create an animated video with text and audio """
    print("Creating animated video...")

    # Get audio duration to determine video length
    audio = AudioFileClip(audio_file)
    audio_duration = audio.duration

    # Video dimensions
    width, height = 1280, 720

    # Split text into sentences for individual slides
    sentences = [s.strip() for s in text.replace('\n', ' ').split('.') if s.strip()]
    if not sentences:
        sentences = ["No text generated"]

    # Calculate how long each sentence should be displayed
    sentence_duration = audio_duration / len(sentences)

    # Create a function that returns the frame at time t
    def make_frame(t):
        # Determine which sentence to show based on time
        sentence_idx = min(int(t / sentence_duration), len(sentences) - 1)
        sentence = sentences[sentence_idx]

        # Get time within current sentence (for fade effects)
        local_t = t - (sentence_idx * sentence_duration)
        fade_duration = 0.5

        # Create base frame with text
        frame = create_text_image(sentence, width, height, style)

        # Apply fade in/out effect
        if local_t < fade_duration:
            # Fade in
            alpha = local_t / fade_duration
            frame = frame * alpha
        elif local_t > sentence_duration - fade_duration:
            # Fade out
            alpha = (sentence_duration - local_t) / fade_duration
            frame = frame * alpha

        return frame.astype('uint8')

    # Create video clip
    video_clip = VideoClip(make_frame, duration=audio_duration)

    # Add audio
    final_video = video_clip.set_audio(audio)

    # Write the result to a file
    final_video.write_videofile(output_file, fps=24, codec='libx264')

    print(f"‚úÖ Video saved to {output_file}!")
    return output_file



def generate_video_from_prompt(prompt="Python is a programming language that",
                              output_path="output_video.mp4",
                              max_text_length=100,
                              style="modern"):
    """Generate a video with animated text and speech from a prompt"""
    print(f"Starting pipeline with prompt: '{prompt}'")
    start_time = time.time()

    # Step 1: Generate text
    generated_text = generate_text(prompt, max_text_length)
    print("\n===== Generated Text =====")
    print(generated_text)
    print("=========================\n")

    # Step 2: Convert text to speech
    audio_file = text_to_speech(generated_text)

    # Step 3: Create animated video with text and audio
    video_file = create_animated_video(generated_text, audio_file, output_path, style)

    result = {
        "text": generated_text,
        "audio": audio_file,
        "video": video_file
    }

    elapsed_time = time.time() - start_time
    print(f"Video generation complete in {elapsed_time:.2f} seconds! Your video is saved at: {result['video']}")
    return result




# Function to display video
def show_video(file_path):
    try:
        from IPython.display import HTML
        from base64 import b64encode

        mp4 = open(file_path, 'rb').read()
        data_url = f"data:video/mp4;base64,{b64encode(mp4).decode()}"
        return HTML(f"""
        <video width="640" height="360" controls>
            <source src="{data_url}" type="video/mp4">
        </video>
        """)
    except ImportError:
        print(f"Video saved at {file_path}. To display in notebook, make sure IPython is installed.")
        return None

def generate_avatar_video(script, avatar_image_path, output_path="avatar_video.mp4", api_key=None):
    """Generate a talking avatar video using D-ID API"""
    if api_key is None:
        api_key = input("Please enter your D-ID API key: ")

    print("Generating avatar video with D-ID API...")

    # D-ID API
    api_url = "https://api.d-id.com/talks"

    # Convert avatar image to base64
    with open(avatar_image_path, "rb") as image_file:
        image_data = image_file.read()
    image_base64 = base64.b64encode(image_data).decode("utf-8")

    # Prepare request payload
    payload = {
        "script": {
            "type": "text",
            "input": script,
            "ssml": False
        },
        "source_base64": image_base64,
        "config": {
            "fluent": True,
            "pad_audio": 0.0
        }
    }

    # Set headers with API key
    headers = {
        'Authorization': f'Basic {api_key}',
        'Content-Type': 'application/json'
    }

    # Send request to D-ID API
    response = requests.post(api_url, headers=headers, json=payload)

    if response.status_code == 201:  # 201 Created indicates success
        print("Video generation initiated successfully!")
        response_data = response.json()
        print(f"Talk ID: {response_data.get('id')}")

        # Get the result URL when ready
        talk_id = response_data.get('id')
        result_url = f"{api_url}/{talk_id}"

        # Poll until video is ready
        print("Waiting for video to be ready...")
        status = "created"
        while status not in ["done", "failed"]:
            time.sleep(5)  # Wait 5 seconds between checks
            status_response = requests.get(result_url, headers={'Authorization': f'Basic {api_key}'})
            if status_response.status_code == 200:
                status_data = status_response.json()
                status = status_data.get('status')
                print(f"Status: {status}")
            else:
                print(f"Error checking status: {status_response.status_code}")
                break

        if status == "done":
            # Download the video
            result_video_url = status_data.get('result_url')
            print(f"Downloading video from: {result_video_url}")
            video_response = requests.get(result_video_url)
            if video_response.status_code == 200:
                with open(output_path, 'wb') as video_file:
                    video_file.write(video_response.content)
                print(f"‚úÖ Avatar video saved to {output_path}!")
                return output_path
            else:
                print(f"Error downloading video: {video_response.status_code}")
        else:
            print("Video generation failed or timed out.")
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

    return None

def create_combined_video(text_video_path, avatar_video_path, output_path="combined_video.mp4"):
    """Combine the text video and avatar video side by side"""
    print("Combining videos...")

    # Load videos
    text_video = VideoFileClip(text_video_path)
    avatar_video = VideoFileClip(avatar_video_path)

    # Make both videos the same duration
    duration = min(text_video.duration, avatar_video.duration)
    text_video = text_video.subclip(0, duration)
    avatar_video = avatar_video.subclip(0, duration)

    # Resize avatar video to match text video height
    text_height = text_video.size[1]
    avatar_video = avatar_video.resize(height=text_height)

    # Composite videos side by side
    final_width = text_video.size[0] + avatar_video.size[0]
    final_height = text_height

    # Create composite
    combined = clips_array([[text_video, avatar_video]])

    # Write to file
    combined.write_videofile(output_path)
    print(f"‚úÖ Combined video saved to {output_path}!")
    return output_path

def main():
    """Main function to run the entire pipeline"""
    # Check and install dependencies
    install_dependencies()

    # Get input from user
    prompt = input("Enter a prompt for text generation (e.g., 'How to learn Python'): ")
    max_length = int(input("Enter maximum text length (recommended: 100-300): ") or "200")
    style = input("Choose style (modern/classic): ") or "modern"
    avatar_image_path = input("Enter path to avatar image file: ")
    api_key = input("Enter your D-ID API key: ")

    # Generate text video
    print("\n=== GENERATING TEXT VIDEO ===")
    text_result = generate_video_from_prompt(
        prompt=prompt,
        output_path="text_video.mp4",
        max_text_length=max_length,
        style=style
    )

    # Generate avatar video using the same text
    print("\n=== GENERATING AVATAR VIDEO ===")
    avatar_video_path = generate_avatar_video(
        script=text_result["text"],
        avatar_image_path=avatar_image_path,
        output_path="avatar_video.mp4",
        api_key=api_key
    )

    if avatar_video_path:
        # Combine videos (optional)
        print("\n=== COMBINING VIDEOS ===")
        combined_video_path = create_combined_video(
            text_video_path=text_result["video"],
            avatar_video_path=avatar_video_path,
            output_path="combined_video.mp4"
        )

        print("\n=== PROCESS COMPLETE ===")
        print(f"Text video: {text_result['video']}")
        print(f"Avatar video: {avatar_video_path}")
        print(f"Combined video: {combined_video_path}")
    else:
        print("\n=== PROCESS COMPLETE ===")
        print(f"Text video: {text_result['video']}")
        print("Avatar video generation failed.")

if __name__ == "__main__":
    main()

"""shows the video ."""

def show_generated_video(video_path):
    """
    Display a video in Jupyter/Colab notebook or open it with a system player

    Args:
        video_path (str): Path to the video file
    """
    # Try to display in Jupyter/Colab if available
    try:
        from IPython.display import HTML, display
        from base64 import b64encode

        # Read the video file
        with open(video_path, 'rb') as f:
            video_data = f.read()

        # Encode to base64
        b64_video = b64encode(video_data).decode()

        # Create HTML video player
        video_html = f"""
        <video width="640" height="360" controls>
            <source src="data:video/mp4;base64,{b64_video}" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        """

        # Display the video
        display(HTML(video_html))
        print("Video displayed in notebook.")

    except (ImportError, NameError):
        # If not in a notebook environment, try to open using system player
        print(f"Attempting to open video at: {video_path}")
        try:
            import os
            import platform

            system = platform.system()
            if system == 'Darwin':  # macOS
                os.system(f"open {video_path}")
            elif system == 'Windows':
                os.system(f"start {video_path}")
            elif system == 'Linux':
                os.system(f"xdg-open {video_path}")
            else:
                print(f"Video saved at: {video_path}. Please open it manually.")

            print(f"Video should open in your default player. If not, find it at: {video_path}")
        except Exception as e:
            print(f"Could not automatically open video: {e}")
            print(f"Video is saved at: {video_path}")


# Example usage:
# After generating your video, call this function with the path
# show_generated_video("combined_video.mp4")  # For the combined video
# show_generated_video("text_video.mp4")      # For the text-only video
# show_generated_video("avatar_video.mp4")    # For the avatar video

# Quick usage example:
if __name__ == "__main__":
    video_path = input("Enter the path to the video file you want to display: ")
    show_generated_video(video_path)

